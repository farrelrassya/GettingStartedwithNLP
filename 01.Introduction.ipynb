{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNXN5T2XViwOfDOBTa0wEbv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/GettingStartedwithNLP/blob/main/01.Introduction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to NLP\n",
        "\n",
        "Natural language processing (or NLP) is a field that addresses various ways in which computers can deal with natural—that is, human—language. Regardless of your occupation or background, there is a good chance you have heard about NLP before, especially in recent years with the media covering the impressive capabili- ties of intelligent machines that can understand and produce natural language. This is what has brought NLP into the spotlight, and what might have attracted you to this book. You might be a programmer who wants to learn new skills, a machine learning or data science practitioner who realizes there is a lot of potential in processing natural language, or you might be generally interested in how language works and how to process it automatically. Either way, welcome to NLP! This book aims to help you get started with it."
      ],
      "metadata": {
        "id": "J16fJODTLPFX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Timeline: A Journey Through Approaches  \n",
        "\n",
        "**Figure 1.1** illustrates the evolution of Natural Language Processing (NLP) techniques over time. Here’s a breakdown of the key phases:  \n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.1.png\" alt=\"Figure 1.1\" width=\"700\">\n",
        "\n",
        "---\n",
        "\n",
        "## 1. **Rule-Based Approaches (1980s)**  \n",
        "- **Early Days of NLP**:  \n",
        "  Systems relied on hand-written rules (e.g., grammar checks, keyword matching).  \n",
        "  Example: Basic spellcheckers or syntax analyzers.  \n",
        "- **Limitation**:  \n",
        "  Rigid and struggled with ambiguity or new language patterns.  \n",
        "\n",
        "## 2. **Statistical & Machine Learning Approaches (1990s–2000s)**  \n",
        "- **The Web Era**:  \n",
        "  The **World Wide Web** (1990s) exploded with text data, enabling statistical models.  \n",
        "  Example: Spam filters, early search engines.  \n",
        "- **Key Idea**:  \n",
        "  Let data drive decisions using probabilities and algorithms (e.g., SVMs, decision trees).  \n",
        "\n",
        "## 3. **Deep Learning Approaches (2010s–Present)**  \n",
        "- **Hardware Revolution**:  \n",
        "  Advances in **computer hardware** (GPUs, TPUs) made training neural networks feasible.  \n",
        "  Example: Transformers, BERT, ChatGPT.  \n",
        "- **Why It’s Powerful**:  \n",
        "  Models learn complex patterns directly from data, excelling at tasks like translation and context understanding.  \n",
        "\n",
        "---\n",
        "\n",
        "### What Fuelled the Shifts?  \n",
        "- **1990s**: The Web provided vast data for statistical learning.  \n",
        "- **2010s**: Hardware advancements unlocked deep learning’s potential.  \n",
        "\n",
        "### Modern NLP Today:  \n",
        "Combines **all three approaches**:  \n",
        "- **Rules** for simple tasks (e.g., regex).  \n",
        "- **ML** for structured decisions (e.g., classification).  \n",
        "- **Deep Learning** for cutting-edge AI (e.g., chatbots, summarization).  \n"
      ],
      "metadata": {
        "id": "UaFyQHUTP0Lj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# Information Retrieval: How Machines Find What We Need\n",
        "\n",
        "Information search systems solve two critical challenges that would be overwhelming for humans:\n",
        "\n",
        "1. Finding relevant information among vast collections of documents (like searching through an entire filesystem or the internet)\n",
        "2. Identifying the *most* relevant documents from the potentially relevant ones\n",
        "\n",
        "## The Basic Concept of Information Filtering\n",
        "\n",
        "When searching manually through a large collection (like thousands of meeting notes), we would typically use a simple filtering approach:\n",
        "\n",
        "- Look for documents containing specific keywords (e.g., \"meeting\" and \"management\")\n",
        "- Exclude all documents that don't contain these terms\n",
        "\n",
        "This filtering process creates a subset of potentially relevant documents from the larger collection, essentially dividing documents into two categories:\n",
        "- Documents that match our search criteria\n",
        "- Documents that don't match our search criteria\n",
        "\n",
        "The text describes this as \"simple filtering\" - it's the fundamental starting point for more sophisticated information retrieval systems.\n",
        "\n",
        "## Why This Matters\n",
        "\n",
        "Modern search systems save us enormous amounts of time by automating this process. They can rapidly scan billions of documents and apply complex relevance algorithms that go far beyond simple keyword matching.\n",
        "\n",
        "Without these systems, we would be overwhelmed by information and unable to efficiently find what we need - truly like searching for \"a needle in a haystack.\"\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.4.png\" alt=\"Figure 1.4\" width=\"700\">\n"
      ],
      "metadata": {
        "id": "KL6AOahYRHRz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Numerical Representation in Information Retrieval\n",
        "\n",
        "Information retrieval systems use numerical representations to process human language since machines excel at handling numbers rather than natural language.\n",
        "\n",
        "## The Challenge of Machine Understanding\n",
        "\n",
        "While machines are becoming better at processing human language, they don't \"understand\" language the way humans do. When we search for words like \"meeting\" and \"management\" in documents, we:\n",
        "- Have mental representations of these words\n",
        "- Know how they're spelled and how they sound\n",
        "- Can naturally recognize them in text\n",
        "\n",
        "Machines lack these inherent representations, so they need a different approach.\n",
        "\n",
        "## Vector Representations: The Machine's Language\n",
        "\n",
        "To enable machines to process language, we translate words into numerical representations. The most common representation in natural language processing is a **vector**.\n",
        "\n",
        "Vector representations are versatile and can represent:\n",
        "- Individual characters\n",
        "- Words\n",
        "- Entire documents\n",
        "\n",
        "## How Vector Representation Works\n",
        "\n",
        "A vector in this context is similar to:\n",
        "- Vectors from high school mathematics\n",
        "- Arrays in programming languages\n",
        "\n",
        "For a simple query like \"management meeting\":\n",
        "1. Each word gets its own dimension in the vector (or cell in an array)\n",
        "2. The \"management\" dimension stores information related to \"management\"\n",
        "3. The \"meeting\" dimension stores information related to \"meeting\"\n",
        "\n",
        "This creates a structured numerical representation that machines can process effectively.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.5.png\" alt=\"Figure 1.5\" width=\"700\">\n",
        "\n",
        "\n",
        "This numerical transformation allows machines to perform complex operations on text data, enabling the search capabilities we rely on daily."
      ],
      "metadata": {
        "id": "pzm6dPu1SCwI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Vector Representation in Information Retrieval with Code Implementation\n",
        "\n",
        "## Quantifying Word Importance\n",
        "\n",
        "When representing documents and queries as vectors, we need to determine how to quantify each word's contribution. For a simple query like \"management meeting\", both terms contribute equally to the information need, which we can represent by their frequency count:\n",
        "\n",
        "- The query \"management meeting\" contains each word exactly once\n",
        "- This gives us a vector representation of [1, 1]\n",
        "- Graphically, this creates a point with coordinates (1,1) in a 2D space\n",
        "\n",
        "## Representing Documents\n",
        "\n",
        "Documents are represented using the same approach - by counting word occurrences:\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.7.png\" alt=\"Figure 1.7\" width=\"700\">\n",
        "\n",
        "\n",
        "\n",
        "### Document Examples\n",
        "- **Doc1**: Contains 3 occurrences of \"management\" and 5 occurrences of \"meeting\"\n",
        "  - Vector representation: [3, 5]\n",
        "  - This creates a point with coordinates (3,5) in our vector space\n",
        "\n",
        "- **Doc2**: Contains 4 occurrences of \"management\" and 1 occurrence of \"meeting\"\n",
        "  - Vector representation: [4, 1]\n",
        "  - This creates a point with coordinates (4,1) in our vector space\n",
        "\n",
        "## The Process Explained\n",
        "\n",
        "1. We start with an empty vector (array) containing zeros for each dimension (word)\n",
        "2. We read through the document, treating text between whitespaces as individual words\n",
        "3. When we encounter \"management\", we increment the count in the first position (index 0)\n",
        "4. When we encounter \"meeting\", we increment the count in the second position (index 1)\n",
        "5. The final vector contains the total count of each word in the document\n",
        "\n",
        "## Implementation Notes\n",
        "\n",
        "- The code provided is designed to work in Jupyter Notebooks\n",
        "- This same approach can be applied to any document by changing the input text\n",
        "- Python arrays are zero-indexed, which is why \"management\" counts are stored at position 0\n",
        "- The same code could be used to create a vector for Doc2, which would result in [4, 1]\n",
        "\n",
        "This numerical representation transforms text into a mathematical form that computers can process efficiently, enabling information retrieval systems to perform operations like measuring document similarity and relevance."
      ],
      "metadata": {
        "id": "BTdqo2lSTtrL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc1 = \"meeting ... management ... meeting ... management ... meeting \"\n",
        "doc1 += \"... management ... meeting ... meeting\"\n",
        "\n",
        "vector = [0, 0]\n",
        "\n",
        "for word in doc1.split(\" \"):\n",
        "    if word==\"management\":\n",
        "        vector[0] = vector[0] + 1\n",
        "    if word==\"meeting\":\n",
        "        vector[1] = vector[1] + 1\n",
        "\n",
        "print (vector)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-ZltNOkK32u",
        "outputId": "ced41443-af66-4902-b79d-654cf9cbe030"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finding Relevant Documents Through Vector Distance\n",
        "\n",
        "## Moving Beyond Basic Vector Representation\n",
        "\n",
        "While our simple vector representation is a powerful starting point, real-world applications require more sophisticated approaches:\n",
        "\n",
        "1. We need to accommodate any query, not just predefined words like \"management\" or \"meeting\"\n",
        "2. We need proper word detection in text (not just simple string matching)\n",
        "3. We need to automatically identify vector dimensions rather than hardcoding them\n",
        "4. We need scalability beyond fixed-size arrays\n",
        "\n",
        "These improvements will be addressed in detail in later chapters. For now, let's focus on how vector representations help us determine document relevance.\n",
        "\n",
        "## From Representation to Relevance\n",
        "\n",
        "The fundamental question: Once we have our documents and query represented as vectors, how do we determine which document is most relevant to the query?\n",
        "\n",
        "This is where the geometric interpretation of vectors becomes incredibly useful. In our vector space:\n",
        "- Each document and query is a point with specific coordinates\n",
        "- Similar documents (with similar word frequencies) will be positioned close together\n",
        "- The most relevant document to a query should be the one closest to it in this vector space\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.8.png\" alt=\"Figure 1.8\" width=\"700\">\n",
        "\n",
        "\n",
        "## Measuring Distance in Vector Space\n",
        "\n",
        "To find the most relevant document, we need a way to measure the distance between points in our vector space. This is where the Euclidean distance formula comes in.\n",
        "\n",
        "The Euclidean distance between two points is calculated using the Pythagorean theorem:\n",
        "\n",
        "$$\n",
        "ED(p, q) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- p and q are the two points (vectors)\n",
        "- p₁, p₂, etc. are the coordinates of point p\n",
        "- q₁, q₂, etc. are the coordinates of point q\n",
        "\n",
        "## Calculating Document Relevance\n",
        "\n",
        "Let's apply this to our example:\n",
        "- Query = [1, 1] (coordinates in the \"management\" and \"meeting\" dimensions)\n",
        "- Doc1 = [3, 5]\n",
        "- Doc2 = [4, 1]\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.9.png\" alt=\"Figure 1.9\" width=\"700\">\n",
        "\n",
        "\n",
        "For the distance between Query and Doc1:\n",
        "- Difference in \"management\" dimension: 3 - 1 = 2\n",
        "- Difference in \"meeting\" dimension: 5 - 1 = 4\n",
        "- ED(Query, Doc1) = √(2² + 4²) = √(4 + 16) = √20 ≈ 4.47\n",
        "\n",
        "For the distance between Query and Doc2:\n",
        "- Difference in \"management\" dimension: 4 - 1 = 3\n",
        "- Difference in \"meeting\" dimension: 1 - 1 = 0\n",
        "- ED(Query, Doc2) = √(3² + 0²) = √9 = 3\n",
        "\n",
        "Since Doc2 has a smaller Euclidean distance to the Query (3 < 4.47), we consider Doc2 more relevant to the Query than Doc1.\n",
        "\n",
        "## Extending to Higher Dimensions\n",
        "\n",
        "The beauty of this approach is that it scales to any number of dimensions. For a vector space with n dimensions (representing n different words), the formula remains the same:\n",
        "\n",
        "$$\n",
        "ED(p, q) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2 + \\cdots + (q_n - p_n)^2}\n",
        "$$\n",
        "\n",
        "This allows information retrieval systems to represent documents with thousands or millions of unique words, and still calculate relevance using the same fundamental principles.\n",
        "\n",
        "By converting text into numerical vector representations and using distance measurements, computers can effectively determine document relevance without truly \"understanding\" language in the human sense."
      ],
      "metadata": {
        "id": "aoLBJdRcW2f7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "query = [1, 1]\n",
        "doc1 = [3, 5]\n",
        "sq_length = 0\n",
        "\n",
        "for index in range(0, len(query)):\n",
        "    sq_length += math.pow((doc1[index] - query[index]), 2)\n",
        "\n",
        "print (math.sqrt(sq_length))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Atb2elUVWmMl",
        "outputId": "4b48f7f9-05e6-48f9-cb42-eec8fff5610e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4.47213595499958\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our Euclidean distance estimation tells us that Doc2 is closer in space to the query than Doc1, so it is more similar, right? Well, there’s one more point that we are missing at the moment. Note that if we typed in management and meeting multiple times in our query, the content and information need would not change, but the vector itself would. In par- ticular, the length of the vector will be different, but the angle between the first version of the vector and the second one won’t change, as you can see in figure 1.10."
      ],
      "metadata": {
        "id": "OFECXPVPa6dG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.10.png\" width=\"700\">"
      ],
      "metadata": {
        "id": "gG809lrmomei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  Cosine Similarity for Better Document Comparison\n",
        "\n",
        "The Problem with Euclidean Distance\n",
        "\n",
        "While Euclidean distance provides a useful way to measure the distance between document vectors, it has a significant limitation: it's affected by document length.\n",
        "\n",
        "Longer documents naturally have higher word count values in their vectors, without necessarily being more relevant. For example:\n",
        "- A longer document may contain more occurrences of words simply because it contains more words overall\n",
        "- Repeating the same query multiple times would create a vector with larger values, but the information content remains the same\n",
        "\n",
        "This creates a bias where longer documents might appear less relevant simply due to their length, not their actual content similarity.\n",
        "\n",
        "### Angle vs. Distance: A Better Approach\n",
        "\n",
        "Rather than measuring the absolute distance between vectors, a more effective approach is to measure the angle between length-normalized vectors:\n",
        "\n",
        "- When two documents contain similar proportions of words (regardless of document length), the angle between their vectors will be small\n",
        "- When documents contain different word distributions, the angle will be large\n",
        "- The angle between identical content (even if repeated) will be zero\n",
        "\n",
        "This makes angle measurement a much more stable and meaningful way to compare documents than raw distance.\n",
        "\n",
        "### Cosine Similarity: Measuring Vector Angles\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/01.%20Chapter%2001/Figure%201.11.png\" width=\"700\">\n",
        "\n",
        "The cosine similarity metric calculates the cosine of the angle between two vectors, providing a measure of their directional similarity regardless of their magnitudes:\n",
        "\n",
        "- Cosine of 0° = 1 (maximum similarity: vectors point in exactly the same direction)\n",
        "- Cosine of 90° = 0 (no similarity: vectors are perpendicular)\n",
        "- Cosine of 180° = -1 (maximum dissimilarity: vectors point in opposite directions)\n",
        "\n",
        "<div style=\"background-color: #E7F3FE; border-left: 6px solid #2196F3; padding: 16px; margin: 16px 0;\">\n",
        "  <h3 style=\"margin-top: 0; color: #2196F3;\">Cosine Similarity</h3>\n",
        "  <p style=\"margin: 0;\">\n",
        "    Cosine similarity estimates the similarity between two nonzero vectors in space (or two texts\n",
        "    represented by such vectors) on the basis of the angle between these vectors. For example, the cosine\n",
        "    of 0° equals 1, indicating maximum similarity, while the cosine of 180° equals –1, the lowest value.\n",
        "    Unlike Euclidean distance, this measure is not affected by vector length.\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "\n",
        "### Key Properties of Cosine Similarity:\n",
        "\n",
        "1. **Range**: Values range from -1 (completely opposite) to 1 (identical)\n",
        "2. **Length Independence**: Not affected by vector magnitude (document length)\n",
        "3. **Proportionality**: Measures whether vectors have similar proportions of values across dimensions\n",
        "4. **Intuitive**: Higher values indicate greater similarity\n"
      ],
      "metadata": {
        "id": "P3VnOtoNorwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculating Cosine Similarity Between Document Vectors\n",
        "\n",
        "### Understanding Vector Relationships\n",
        "\n",
        "When comparing documents as vectors, understanding their directional relationship provides insights about their content similarity:\n",
        "\n",
        "- **Orthogonal vectors (90° angle, cosine = 0)**: Represent completely different content with no overlap. For example, if vector1 represents a query with only the word \"management\" and vector2 represents a query with only the word \"meeting\", they share no common terms.\n",
        "\n",
        "- **Opposing vectors (180° angle, cosine = -1)**: In word occurrence-based vectors, this rarely occurs as word counts cannot be negative. The cosine similarity for document vectors will typically range from 0 to 1.\n",
        "\n",
        "### Calculating Cosine Similarity\n",
        "\n",
        "Cosine similarity is calculated using the dot product of two vectors divided by the product of their lengths:\n",
        "\n",
        "```\n",
        "cosine(vec1, vec2) = dot_product(vec1, vec2) / (length(vec1) * length(vec2))\n",
        "```\n",
        "\n",
        "This is derived from the Euclidean definition of dot product:\n",
        "\n",
        "```\n",
        "dot_product(vec1, vec2) = length(vec1) * length(vec2) * cosine(vec1, vec2)\n",
        "```\n",
        "\n",
        "### The Dot Product\n",
        "\n",
        "The dot product is simply the sum of the coordinate products of two vectors along each dimension:\n",
        "\n",
        "```\n",
        "dot_product(query, Doc1) = 1*3 + 1*5 = 8\n",
        "dot_product(query, Doc2) = 1*4 + 1*1 = 5\n",
        "dot_product(Doc1, Doc2) = 3*4 + 5*1 = 17\n",
        "```\n",
        "\n",
        "### Vector Length\n",
        "\n",
        "The length (magnitude) of a vector is calculated using the Euclidean distance formula from the origin (0,0):\n",
        "\n",
        "```\n",
        "length(query) = √((1-0)² + (1-0)²) ≈ 1.41\n",
        "length(Doc1) = √((3-0)² + (5-0)²) ≈ 5.83\n",
        "length(Doc2) = √((4-0)² + (1-0)²) ≈ 4.12\n",
        "```\n",
        "\n",
        "### Final Cosine Similarity Calculations\n",
        "\n",
        "Using these components, we can calculate the cosine similarity:\n",
        "\n",
        "```\n",
        "cos(query, Doc1) = dot_product(query, Doc1) / (length(query) * length(Doc1))\n",
        "                  = 8 / (1.41 * 5.83) ≈ 0.97\n",
        "\n",
        "cos(query, Doc2) = dot_product(query, Doc2) / (length(query) * length(Doc2))\n",
        "                  = 5 / (1.41 * 4.12) ≈ 0.86\n",
        "```"
      ],
      "metadata": {
        "id": "ykHGkBneqS5e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "query = [1, 1]\n",
        "doc1 = [3, 5]\n",
        "\n",
        "def length(vector):\n",
        "    sq_length = 0\n",
        "    for index in range(0, len(vector)):\n",
        "        sq_length += math.pow(vector[index], 2)\n",
        "    return math.sqrt(sq_length)\n",
        "\n",
        "def dot_product(vector1, vector2):\n",
        "    if len(vector1) == len(vector2):\n",
        "        dot_prod = 0\n",
        "        for index in range(0, len(vector1)):\n",
        "            dot_prod += vector1[index] * vector2[index]\n",
        "        return dot_prod\n",
        "    else:\n",
        "        return \"Unmatching dimensionality\"\n",
        "\n",
        "cosine = dot_product(query, doc1) / (length(query) * length(doc1))\n",
        "print(cosine)  # Output: approximately 0.97"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2p1pC6OXZFwG",
        "outputId": "971486ef-cc8e-4534-d914-c6df9ba969ad"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.9701425001453319\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interpreting the Results\n",
        "\n",
        "Our calculations show that:\n",
        "- Doc1 has a cosine similarity of 0.97 with the query\n",
        "- Doc2 has a cosine similarity of 0.86 with the query\n",
        "\n",
        "Despite Doc2 being closer in Euclidean distance (as we calculated earlier), Doc1 is actually more similar to the query when we measure by content distribution. This is because the content in both the query and Doc1 is more evenly balanced between \"management\" and \"meeting,\" while Doc2 is heavily weighted toward \"management\" with only one mention of \"meeting.\"\n",
        "\n",
        "This demonstrates why cosine similarity is often a better measure for document relevance than Euclidean distance - it captures the proportional distribution of content rather than being influenced by absolute term frequencies or document length."
      ],
      "metadata": {
        "id": "SR_Nd_Y8qmNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Let's say each position corresponds to counts of [\"management\", \"meeting\"].\n",
        "query = np.array([2, 2])   # e.g., management=2, meeting=2\n",
        "doc1  = np.array([1, 1])   # management=1, meeting=1\n",
        "doc2  = np.array([3, 2])   # management=3, meeting=2\n",
        "doc3 = 2 * doc1             # Doc3 is Doc1 scaled (double the term frequencies)\n",
        "doc4 = 2 * doc2\n",
        "\n",
        "def euclidean_distance(a, b):\n",
        "    return np.linalg.norm(a - b)\n",
        "\n",
        "def cosine_similarity(a, b):\n",
        "    eps = 1e-10\n",
        "    return np.dot(a, b) / (np.linalg.norm(a) * np.linalg.norm(b) + eps)\n",
        "\n",
        "documents = {\n",
        "    \"Doc1\": doc1,\n",
        "    \"Doc2\": doc2,\n",
        "    \"Doc3 (2x Doc1)\": doc3,\n",
        "    \"Doc4 (2x Doc2)\": doc4\n",
        "}\n",
        "\n",
        "print(\"Euclidean Distances from Query:\")\n",
        "for name, vec in documents.items():\n",
        "    print(f\"{name}: {euclidean_distance(query, vec):.3f}\")\n",
        "\n",
        "print(\"\\nCosine Similarities with Query:\")\n",
        "for name, vec in documents.items():\n",
        "    print(f\"{name}: {cosine_similarity(query, vec):.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwtOwBMgqYrd",
        "outputId": "7413c2cf-1686-4734-aaef-05cf8f26c0dd"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclidean Distances from Query:\n",
            "Doc1: 1.414\n",
            "Doc2: 1.000\n",
            "Doc3 (2x Doc1): 0.000\n",
            "Doc4 (2x Doc2): 4.472\n",
            "\n",
            "Cosine Similarities with Query:\n",
            "Doc1: 1.000\n",
            "Doc2: 0.981\n",
            "Doc3 (2x Doc1): 1.000\n",
            "Doc4 (2x Doc2): 0.981\n"
          ]
        }
      ]
    }
  ]
}