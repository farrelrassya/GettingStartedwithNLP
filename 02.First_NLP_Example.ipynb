{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO48KEF6M5LGkfJT7rPWxC8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/farrelrassya/GettingStartedwithNLP/blob/main/02.First_NLP_Example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Your first NLP example\n",
        "\n",
        "In this chapter, you will learn how to implement your own NLP application from scratch. In doing so, you will also learn how to structure a typical NLP pipeline and how to apply a simple machine learning algorithm to solve your task. The particular application you will implement is spam filtering. We overviewed it in chapter 1 as one of the classic tasks on the intersection of NLP and machine learning."
      ],
      "metadata": {
        "id": "KPq27IN525bT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introducing NLP in practice: Spam filtering\n",
        "In this book, you use spam filtering as your first practical NLP application, as it exemplifies a widely spread family of tasks—text classification. Text classification comprises several applications that we discuss in this book, including user profiling"
      ],
      "metadata": {
        "id": "Cxr3KGH02_SY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We apply classification in our everyday lives pretty regularly: classifying things simply implies that we try to put them into clearly defined groups, classes, or categories. In fact, we tend to classify all sorts of things all the time. Here are some examples:\n",
        "1.  Based on our level of engagement and interest in a movie, we may classify it as interesting or boring.\n",
        "2. Based on temperature, we classify water as cold or hot.\n",
        "3. Based on the amount of sunshine, humidity, wind strength, and air tempera-\n",
        "ture, we classify the weather as good or bad.\n",
        "4. Based on the number of wheels, we classify vehicles into unicycles, bicycles, tri-\n",
        "cycles, quadricycles, cars, and so on.\n",
        "5. Based on the availability of the engine, we may classify two-wheeled vehicles into bicycles and motorcycles."
      ],
      "metadata": {
        "id": "GjEpblLy3GPA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding the task\n",
        "Consider the following scenario: you have a collection of spam and normal emails from the past. You are tasked with building a spam filter, which for any future incoming email can predict whether this email is spam or not. Consider these questions:\n",
        "1. How can you use the provided data?\n",
        "2. What characteristics of the emails might be particularly useful, and how will you\n",
        "extract them?\n",
        "3. What will be the sequence of steps in this application?\n",
        "In this section, we will discuss this scenario and look into the implementation steps. In total, the pipeline for this task will consist of five steps, visualized as a flow chart in figure 2.4. Now let’s look into each of these steps in more detail.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.4.png\" style=\"width:700px;\">\n"
      ],
      "metadata": {
        "id": "AQ7J4eWEHTeK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 1: Define the data and classes\n",
        "First, you need to ask yourself what format the email messages are delivered in for this task. For instance, in a real-life situation, you might need to extract the mes- sages from the mail agent application. However, for simplicity, let’s assume that someone has extracted the emails for you and stored them in text format. The nor- mal emails are stored in a separate folder—let’s call it Ham, and spam emails are stored in a Spam folder.\n",
        "\n",
        "<div style=\"background-color: #E7F3FE; border-left: 6px solid #2196F3; padding: 16px; margin: 16px 0;\">\n",
        "  <h3 style=\"margin-top: 0; color: #2196F3;\">Note</h3>\n",
        "  <p style=\"margin: 0;\">\n",
        "    If you are wondering why “normal” emails are sometimes referred to as “ham” within the context of spam detection, please review the historical background of the term “spam” for further clarification. For example, you may consult the definition provided by <a href=\"https://www.merriam-webster.com/dictionary/spam\" target=\"_blank\" style=\"color: #2196F3;\">Merriam-Webster's Dictionary</a>.\n",
        "  </p>\n",
        "</div>\n"
      ],
      "metadata": {
        "id": "UUS6ayVB3ZMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In cases where past spam and ham emails have already been identified (for instance, by extracting emails from the INBOX and SPAM folders), manual labeling is unnecessary. However, you must still instruct the machine-learning algorithm by clearly specifying which folder corresponds to ham and which to spam. This initial step—defining class labels and determining the number of classes—is essential for any spam-detection or text-classification pipeline, as it lays the groundwork for subsequent stages such as data preprocessing, feature extraction, and model training and testing (see Figure 2.5).\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.5.png\" style=\"width:700px;\">\n"
      ],
      "metadata": {
        "id": "AdK2k7YV4Yz6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 2: Split the text into words\n",
        "Next, you will need to define the features for the machine to know what type of infor- mation, or what properties of the emails to pay attention to, but before you can do that, there is one more step to perform. As we’ve just discussed in the previous exer- cise, email content provides significant information as to whether an email is ham or spam."
      ],
      "metadata": {
        "id": "0NxbnJU04c8v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One approach to extract email content is to treat the entire email as a single textual feature, such as using the full text of meeting minutes for ham emails or the complete body of a spam email. Although this method allows the algorithm to identify emails based on exact phrase matches, it is unlikely to encounter identical texts repeatedly, and even a slight variation in characters could alter the feature significantly. Consequently, a more effective strategy is to use smaller text segments, like individual words, as features. These words not only convey spam-related information (e.g., \"lottery\" as a potential indicator of spam) but are also more likely to appear across multiple emails, enhancing the algorithm's robustness."
      ],
      "metadata": {
        "id": "1Vv4GhNR404v"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sSFSlVlyM9q",
        "outputId": "7ae97d0e-eb47-4445-b925-894716afc3a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Define\n",
            "which\n",
            "data\n",
            "represents\n",
            "each\n",
            "class\n",
            "for\n",
            "the\n",
            "machine\n",
            "learning\n",
            "algorithm\n"
          ]
        }
      ],
      "source": [
        "text = \"Define which data represents each class for the machine learning algorithm\"\n",
        "words = text.split()  # By default, split() uses any whitespace as the delimiter\n",
        "\n",
        "for word in words:\n",
        "    print(word)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.6.png\" style=\"width:700px;\">"
      ],
      "metadata": {
        "id": "PzzwjXmlArM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Text Tokenization and Punctuation Handling\n",
        "\n",
        "### The Problem\n",
        "\n",
        "When text is split only by whitespace, punctuation marks remain attached to words, resulting in tokens like \"algorithm.\" that differ from their punctuation-free counterparts like \"algorithm\".\n",
        "\n",
        "### Solution Approaches\n",
        "\n",
        "To address this issue, modify the splitting strategy so that punctuation marks are separated from words. This can be achieved through two main methods:\n",
        "\n",
        "1. **Using Python's Regular Expressions module**\n",
        "   - Provides powerful pattern matching capabilities\n",
        "   - Can separate punctuation with specific regex patterns\n",
        "\n",
        "2. **Implementing a simple iterative algorithm**\n",
        "   - Examine each character sequentially\n",
        "   - If character is whitespace:\n",
        "     - Add current word to token list\n",
        "     - Reset current word\n",
        "   - If character is punctuation:\n",
        "     - Treat as separate token\n",
        "     - Handle appropriately based on context (e.g., following whitespace)\n",
        "\n",
        "### Importance\n",
        "\n",
        "This improved tokenization method ensures more accurate text processing, which is crucial for:\n",
        "- More precise text analysis\n",
        "- Better performance in downstream tasks (e.g., spam detection)\n",
        "- Consistent token identification regardless of punctuation"
      ],
      "metadata": {
        "id": "qMblacxf48OS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Character-by-Character Tokenization Algorithm\n",
        "\n",
        "### Algorithm Description\n",
        "The algorithm processes text one character at a time:\n",
        "\n",
        "- When encountering **whitespace** with a non-empty current word:\n",
        "  - Add the current word to the token list\n",
        "  - Reset the current word buffer\n",
        "\n",
        "- When encountering **punctuation**:\n",
        "  - If current word is empty:\n",
        "    - Add just the punctuation mark as a token\n",
        "  - If current word is non-empty:\n",
        "    - Add the current word as a token\n",
        "    - Add the punctuation mark as a separate token\n",
        "    - Reset the current word buffer\n",
        "\n",
        "### Limitations\n",
        "This approach successfully generates a list of words and punctuation tokens, but has important limitations:\n",
        "\n",
        "- May incorrectly split abbreviations and special cases:\n",
        "  - \"i.e.\" → [\"i\", \".\", \"e\", \".\"]\n",
        "  - \"U.S.A.\" → [\"U\", \".\", \"S\", \".\", \"A\", \".\"]\n",
        "\n",
        "### Recommendation\n",
        "For handling special cases and exceptions more effectively, consider:\n",
        "- Advanced tokenization methods using regular expressions\n",
        "- NLP toolkits with specialized tokenization capabilities\n",
        "- Custom rules for common abbreviations and domain-specific terms"
      ],
      "metadata": {
        "id": "nrzCtuyS5Kc7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Define which data represents \"ham\" class and which data represents \"spam\" class for the machine learning algorithm.'\n",
        "delimiters = ['\"', '.']  # List of punctuation marks to be treated as separate tokens\n",
        "\n",
        "words = []       # List to store the resulting tokens\n",
        "current_word = \"\"  # Variable to build up characters of the current word\n",
        "\n",
        "for char in text:\n",
        "    if char == \" \":\n",
        "        # When a whitespace is encountered, add the current word (if not empty) to the tokens list\n",
        "        if current_word != \"\":\n",
        "            words.append(current_word)\n",
        "            current_word = \"\"\n",
        "    elif char in delimiters:\n",
        "        # If a punctuation mark is encountered and a word is being built, append both the word and punctuation\n",
        "        if current_word != \"\":\n",
        "            words.append(current_word)\n",
        "            words.append(char)\n",
        "            current_word = \"\"\n",
        "        else:\n",
        "            # If no word is being built, just add the punctuation as a separate token\n",
        "            words.append(char)\n",
        "    else:\n",
        "        # For any other character, add it to the current word\n",
        "        current_word += char\n",
        "\n",
        "# After the loop, add any remaining word to the tokens list\n",
        "if current_word != \"\":\n",
        "    words.append(current_word)\n",
        "\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eUqvzZw94o8s",
        "outputId": "42f8bc20-15b9-43ea-9d69-f69864951b1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Define', 'which', 'data', 'represents', '\"', 'ham', '\"', 'class', 'and', 'which', 'data', 'represents', '\"', 'spam', '\"', 'class', 'for', 'the', 'machine', 'learning', 'algorithm', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"background-color: #E7F3FE; border-left: 6px solid #2196F3; padding: 16px; margin: 16px 0;\">\n",
        "  <h3 style=\"margin-top: 0; color: #2196F3;\">Tokenization</h3>\n",
        "  <p style=\"margin: 0;\">\n",
        "    Tokenization is the process of identifying or extracting individual words or tokens from a continuous stream of text. As the first step in text preprocessing, it plays a critical role in natural language processing (NLP). Although whitespace and punctuation marks typically act as effective delimiters, there are notable exceptions, such as abbreviations like “U.S.A.”, which require more refined handling. Tokenizers are specialized NLP tools that often utilize carefully constructed regular expressions or advanced machine learning algorithms to manage these complexities efficiently.\n",
        "  </p>\n",
        "</div>"
      ],
      "metadata": {
        "id": "Qk_KjD_c_qCQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let’s now define step 2 of your algorithm as follows: apply tokenization to split the running text into words, which are going to serve as features (figure 2.7).\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.7.png\" style=\"width:700px;\">\n"
      ],
      "metadata": {
        "id": "0b3DCkR8_vOW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 3: Extract and normalize the features\n",
        "Now we look closely into the extracted words and see whether they are all equally good to be used as features—that is, whether they are equally indicative of the spam- related content. Suppose two emails use a different format: one says\n",
        "\n",
        " ``` Collect your lottery winnings ```\n",
        "\n",
        " while another one says\n",
        "\n",
        " ```Collect Your Lottery Winnings ```  "
      ],
      "metadata": {
        "id": "Vgi2_HGUA3a8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The algorithm that splits these messages into words will end up with different word lists because, for instance, lottery ≠ Lottery, but is it different in terms of the meaning? To get rid of such formatting issues like uppercase versus lowercase, you can put all the extracted words into lowercase using Python functionality. Therefore, step 3 in your algorithm should be defined as follows: extract and normalize the features; for example, by putting all words to lowercase (figure 2.8).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.8.png\" style=\"width:700px;\">"
      ],
      "metadata": {
        "id": "QsWtChb-B4N-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 4: Train a classifier\n",
        "At this point, you will end up with two sets of data—one linked to the spam class and another one linked to the ham class. Each data is preprocessed in the same way in steps 2 and 3, and the features are extracted. Next, you need to let the machine use this data to build the connection between the set of features (properties) that describe each type of email (spam or ham) and the labels attached to each type. In step 4, a machine-learning algorithm tries to build a statistical model, a function, that helps it distinguish between the two classes. This is what happens during the learning (training) phase. Figure 2.9 is a refresher visualizing the training and test processes.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.9.png\" style=\"width:700px;\">\n"
      ],
      "metadata": {
        "id": "vBA-UedACA5J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "So, step 4 of the algorithm should be defined as follows: define a machine-learning model and train it on the data with the features predefined in the previous steps (figure 2.10).\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.10.png\" style=\"width:700px;\">\n"
      ],
      "metadata": {
        "id": "FBY6gxZhCkqX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training and Testing Machine Learning Models for Email Classification\n",
        "\n",
        "Once you've built a machine learning model that maps features (like word occurrences) to labels (spam or ham), you need to verify how well it actually performs. This validation process is crucial for ensuring your model will work effectively on new, unseen emails.\n",
        "\n",
        "### Understanding the Need for Separate Testing\n",
        "\n",
        "During training, your algorithm learns which features correlate with each class:\n",
        "- Words like \"lottery\" might strongly indicate spam\n",
        "- Words like \"meeting\" might strongly indicate legitimate (ham) emails\n",
        "\n",
        "However, simply checking performance on the same data used for training creates a misleading evaluation. The model already \"knows\" these answers because it was trained on them. It would be like giving students the same questions for homework and final exam - it doesn't truly test their ability to apply knowledge to new situations.\n",
        "\n",
        "### The Train-Test Split Methodology\n",
        "\n",
        "To properly evaluate model performance, follow these steps:\n",
        "\n",
        "1. **Shuffle your data** to ensure random distribution and avoid any systematic bias\n",
        "   - This prevents situations where all emails of one type might end up in either training or testing\n",
        "\n",
        "2. **Split the dataset** into two separate portions:\n",
        "   - **Training set** (typically 80% of data): Used to train the model and allow it to learn patterns\n",
        "   - **Test set** (typically 20% of data): Reserved exclusively for evaluation\n",
        "\n",
        "3. **Train your classifier** using only the training set\n",
        "   - The model learns to associate features with classes based on this data\n",
        "\n",
        "4. **Evaluate performance** using only the test set\n",
        "   - This provides a realistic estimate of how your model will perform on new, unseen emails\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/02.%20Chapter%2002/Figure%202.11.png\" style=\"width:700px;\">\n",
        "\n",
        "\n",
        "### The Importance of Dataset Separation\n",
        "\n",
        "The training and test sets must remain completely separate throughout the entire process. The test set should never be used during the training phase - it must be treated as \"unseen data\" that the model encounters only during final evaluation.\n",
        "\n",
        "This separation ensures:\n",
        "- A fair assessment of model performance\n",
        "- Detection of overfitting (when a model performs well on training data but poorly on new data)\n",
        "- Realistic expectations for real-world performance\n",
        "\n",
        "By following this methodology, you can trust that your spam classifier's performance metrics are reliable indicators of how it will perform when deployed to filter actual incoming emails."
      ],
      "metadata": {
        "id": "p3JS5dbnC32C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Step 5 involves evaluating the classifier's performance\n",
        "on the test data. This is done by determining the proportion of emails that the classifier correctly labels—assigning the spam label to spam emails and the ham label to non-spam emails. This measure is known as accuracy and is calculated using the following equation:\n",
        "\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{TP + TN}{TP + TN + FP + FN}\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "1. TP (True Positives): Correctly identified spam emails.\n",
        "2. TN (True Negatives): Correctly identified ham emails.\n",
        "3. FP (False Positives): Ham emails incorrectly labeled as spam.\n",
        "4. FN (False Negatives): Spam emails incorrectly labeled as ham.\n",
        "\n",
        "While accuracy provides an overall measure of the classifier's performance, it is essential to also consider the distribution of classes (spam vs. ham) and the individual performance on each class. This ensures that the evaluation captures the strengths and weaknesses of the classifier comprehensively."
      ],
      "metadata": {
        "id": "K5Nvfl01Dxsa"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Slm_QVQA5IRG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementing your own spam filter\n",
        "Now let’s implement each of the five steps. It’s time you open Jupyter and create a new notebook to start coding your own spam filter."
      ],
      "metadata": {
        "id": "70yx2Hw7HH8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download file dari GitHub (gunakan link raw)\n",
        "!wget -O enron1.tar.gz https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/enron1.tar.gz\n",
        "\n",
        "# Ekstrak file tar.gz\n",
        "!tar -xzf enron1.tar.gz\n",
        "\n",
        "# Cek folder hasil ekstrak (opsional)\n",
        "!ls -l"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50xsZVn7MBZd",
        "outputId": "e38f1f2e-fd18-447b-e3ac-cb16f7a9fec8"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-03-16 09:01:49--  https://raw.githubusercontent.com/farrelrassya/GettingStartedwithNLP/main/enron1.tar.gz\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1802573 (1.7M) [application/octet-stream]\n",
            "Saving to: ‘enron1.tar.gz’\n",
            "\n",
            "\renron1.tar.gz         0%[                    ]       0  --.-KB/s               \renron1.tar.gz       100%[===================>]   1.72M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2025-03-16 09:01:49 (24.5 MB/s) - ‘enron1.tar.gz’ saved [1802573/1802573]\n",
            "\n",
            "total 1776\n",
            "-rw-r--r-- 1 root root    1535 Mar 16 08:45 dataset.zip\n",
            "drwx------ 4 1006  513    4096 May 15  2006 enron1\n",
            "-rw-r--r-- 1 root root 1802573 Mar 16 09:01 enron1.tar.gz\n",
            "drwxr-xr-x 1 root root    4096 Mar 13 13:31 sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import codecs\n",
        "\n",
        "def read_in(folder):\n",
        "    \"\"\"\n",
        "    Reads all non-hidden files in the specified folder and returns a list of their contents.\n",
        "\n",
        "    Args:\n",
        "        folder (str): Path to the folder containing files.\n",
        "\n",
        "    Returns:\n",
        "        list: A list where each element is the content of a file.\n",
        "    \"\"\"\n",
        "    files = os.listdir(folder)\n",
        "    contents = []\n",
        "    for file_name in files:\n",
        "        # Skip hidden files (starting with a dot)\n",
        "        if not file_name.startswith('.'):\n",
        "            file_path = os.path.join(folder, file_name)\n",
        "            with codecs.open(file_path, \"r\", encoding=\"ISO-8859-1\", errors=\"ignore\") as f:\n",
        "                contents.append(f.read())\n",
        "    return contents"
      ],
      "metadata": {
        "id": "DGO4IlGfHNcO"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "spam_list = read_in(\"enron1/spam/\")\n",
        "ham_list = read_in(\"enron1/ham/\")\n",
        "\n",
        "# Nge-print jumlah file di folder spam dan ham\n",
        "print(len(spam_list))\n",
        "print(len(ham_list))\n",
        "\n",
        "# Nge-print isi file pertama di masing-masing folder\n",
        "print(spam_list[0])\n",
        "print(ham_list[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02czN-YOHmK5",
        "outputId": "dbf3f831-ee9b-4072-ff54-a771ef0c673c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1500\n",
            "3672\n",
            "Subject: hello paliourg , remember me katie we met online .\r\n",
            "hi paliourg ,\r\n",
            "it ' s katie remember me ? we met online last week . anyways i just signed up to the largest adult dating site ever !\r\n",
            "me and my friends , estelle , adela , and brandi\r\n",
            "are waiting for you ; ) so\r\n",
            "never cackle unless you lay .\r\n",
            "the bigger they are the harder they fall . . silence is less injurious than a bad reply . . nothing is ill said if it is not ill taken . . a fool in a gown is none the wiser . .\r\n",
            "oaks may fall when reeds take the storm .\r\n",
            "too many clicks spoil the browse .\r\n",
            "no more ?\r\n",
            ". better safe than sorry . . true beauty lies within . . a bad excuse is better then none . .\r\n",
            "always you are to be rich next year . . anger and hate hinder good counsel . . an elephant never forgets . .\r\n",
            "fore - warned is fore - armed . . don ' t cross the bridge till you come to it . . variety is the spice of life . .\r\n",
            "\n",
            "Subject: november prelim wellhead production - estimate\r\n",
            "daren ,\r\n",
            "fyi .\r\n",
            "bob\r\n",
            "- - - - - - - - - - - - - - - - - - - - - - forwarded by robert cotten / hou / ect on 10 / 24 / 2000 11 : 56\r\n",
            "am - - - - - - - - - - - - - - - - - - - - - - - - - - -\r\n",
            "vance l taylor\r\n",
            "10 / 24 / 2000 11 : 51 am\r\n",
            "to : robert cotten / hou / ect @ ect\r\n",
            "cc : gary a hanks / hou / ect @ ect , melissa graves / hou / ect @ ect\r\n",
            "subject : november prelim wellhead production - estimate\r\n",
            "bob ,\r\n",
            "here ' s a november preliminary updated file .\r\n",
            "vlt\r\n",
            "x 3 - 6353\r\n",
            "- - - - - - - - - - - - - - - - - - - - - - forwarded by vance l taylor / hou / ect on 10 / 24 / 2000\r\n",
            "11 : 50 am - - - - - - - - - - - - - - - - - - - - - - - - - - -\r\n",
            "vance l taylor\r\n",
            "10 / 23 / 2000 04 : 42 pm\r\n",
            "to : robert cotten / hou / ect @ ect\r\n",
            "cc : gary a hanks / hou / ect @ ect , mary m smith / hou / ect @ ect , lisa\r\n",
            "hesse / hou / ect @ ect , melissa graves / hou / ect @ ect\r\n",
            "subject : november prelim wellhead production - estimate\r\n",
            "bob ,\r\n",
            "please see the attached file estimating wellhead production for the month of\r\n",
            "november . please be advised that this is a preliminary estimate as to this\r\n",
            "date , we have received no noms for november and there may be revisions from\r\n",
            "the traders . i will update you with any revisions as they occur .\r\n",
            "thanks ,\r\n",
            "vlt\r\n",
            "x 3 - 6353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "all_emails = [(email_content, \"spam\") for email_content in spam_list]\n",
        "all_emails += [(email_content, \"ham\") for email_content in ham_list]\n",
        "random.seed(42)\n",
        "random.shuffle(all_emails)\n",
        "print (f\"Dataset size = {str(len(all_emails))} emails\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EpKFdsAKMTwf",
        "outputId": "6b080102-0627-4275-9c13-1c82d4fefa9e"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset size = 5172 emails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "def get_features(text):\n",
        "    features = {}\n",
        "    word_list = [word for word in word_tokenize(text.lower())]\n",
        "    for word in word_list:\n",
        "        features[word] = True\n",
        "    return features\n",
        "\n",
        "all_features = [(get_features(email), label) for (email, label) in all_emails]\n",
        "\n",
        "print(get_features(\"Participate In Our New Lottery NOW!\"))\n",
        "print(len(all_features))\n",
        "print(len(all_features[0][0]))\n",
        "print(len(all_features[99][0]))"
      ],
      "metadata": {
        "id": "7nGaysWVHnoF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a3c2fa5-bbf0-4338-f52f-b4b9d3f6a698"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'participate': True, 'in': True, 'our': True, 'new': True, 'lottery': True, 'now': True, '!': True}\n",
            "5172\n",
            "499\n",
            "41\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import NaiveBayesClassifier, classify\n",
        "\n",
        "def train(features, proportion):\n",
        "    \"\"\"\n",
        "    Splits the given features into training and testing sets and trains a Naive Bayes classifier.\n",
        "\n",
        "    This function divides the feature set into a training set and a test set based on the specified\n",
        "    proportion (e.g., 0.8 for 80% training data). It then trains the Naive Bayes classifier on the training set.\n",
        "\n",
        "    Args:\n",
        "        features (list): A list of feature tuples with labels.\n",
        "        proportion (float): The fraction of the data to use for training (e.g., 0.8 for 80%).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the training set, test set, and the trained classifier.\n",
        "    \"\"\"\n",
        "    train_size = int(len(features) * proportion)\n",
        "    # Initialize the training and test sets\n",
        "    train_set, test_set = features[:train_size], features[train_size:]\n",
        "    print(f\"Training set size = {len(train_set)} emails\")\n",
        "    print(f\"Test set size = {len(test_set)} emails\")\n",
        "\n",
        "    # Train the Naive Bayes classifier using the training set\n",
        "    classifier = NaiveBayesClassifier.train(train_set)\n",
        "    return train_set, test_set, classifier\n",
        "\n",
        "# Example usage:\n",
        "train_set, test_set, classifier = train(all_features, 0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKZM3SHgMirr",
        "outputId": "7068349a-bf20-4a5b-f648-e917ec26c19f"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size = 4137 emails\n",
            "Test set size = 1035 emails\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(train_set, test_set, classifier):\n",
        "    \"\"\"\n",
        "    Evaluates the performance of the given classifier by printing the accuracy on both\n",
        "    the training set and the test set, and displaying the most informative features.\n",
        "\n",
        "    Args:\n",
        "        train_set (list): The training dataset with features and labels.\n",
        "        test_set (list): The testing dataset with features and labels.\n",
        "        classifier (nltk.NaiveBayesClassifier): The trained Naive Bayes classifier.\n",
        "    \"\"\"\n",
        "    # Check accuracy on the training and test sets\n",
        "    print(f\"Accuracy on the training set = {classify.accuracy(classifier, train_set)}\")\n",
        "    print(f\"Accuracy on the test set = {classify.accuracy(classifier, test_set)}\")\n",
        "\n",
        "    # Display the top 50 most informative features\n",
        "    classifier.show_most_informative_features(50)\n",
        "\n",
        "# Call the evaluate function to check classifier performance\n",
        "evaluate(train_set, test_set, classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NR-nLfE6MNIB",
        "outputId": "2ba62055-f9df-4268-8520-e2d11aee40cb"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.957457094512932\n",
            "Accuracy on the test set = 0.9584541062801932\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    197.1 : 1.0\n",
            "                     hou = True              ham : spam   =    186.4 : 1.0\n",
            "                    2004 = True             spam : ham    =    163.0 : 1.0\n",
            "            prescription = True             spam : ham    =    129.3 : 1.0\n",
            "                    pain = True             spam : ham    =     94.0 : 1.0\n",
            "                    2005 = True             spam : ham    =     90.8 : 1.0\n",
            "                    spam = True             spam : ham    =     89.1 : 1.0\n",
            "                     ect = True              ham : spam   =     82.9 : 1.0\n",
            "                  farmer = True              ham : spam   =     81.4 : 1.0\n",
            "                  differ = True             spam : ham    =     77.9 : 1.0\n",
            "                   super = True             spam : ham    =     77.9 : 1.0\n",
            "                featured = True             spam : ham    =     74.7 : 1.0\n",
            "              nomination = True              ham : spam   =     73.5 : 1.0\n",
            "                      ex = True             spam : ham    =     71.5 : 1.0\n",
            "                creative = True             spam : ham    =     68.3 : 1.0\n",
            "             medications = True             spam : ham    =     65.1 : 1.0\n",
            "                     713 = True              ham : spam   =     64.6 : 1.0\n",
            "                     ibm = True             spam : ham    =     61.8 : 1.0\n",
            "                   epson = True             spam : ham    =     57.0 : 1.0\n",
            "                    2001 = True              ham : spam   =     57.0 : 1.0\n",
            "                congress = True             spam : ham    =     55.4 : 1.0\n",
            "                  weight = True             spam : ham    =     55.4 : 1.0\n",
            "              complaints = True             spam : ham    =     53.8 : 1.0\n",
            "                    draw = True             spam : ham    =     53.8 : 1.0\n",
            "             legislation = True             spam : ham    =     53.8 : 1.0\n",
            "                     pro = True             spam : ham    =     53.8 : 1.0\n",
            "                 dealers = True             spam : ham    =     52.2 : 1.0\n",
            "                deciding = True             spam : ham    =     52.2 : 1.0\n",
            "              pertaining = True             spam : ham    =     52.2 : 1.0\n",
            "                   cheap = True             spam : ham    =     50.6 : 1.0\n",
            "                 doctors = True             spam : ham    =     49.0 : 1.0\n",
            "                  health = True             spam : ham    =     47.7 : 1.0\n",
            "                 foresee = True             spam : ham    =     45.8 : 1.0\n",
            "                      cc = True              ham : spam   =     44.4 : 1.0\n",
            "                 advises = True             spam : ham    =     44.2 : 1.0\n",
            "                 beliefs = True             spam : ham    =     44.2 : 1.0\n",
            "                     sum = True             spam : ham    =     44.2 : 1.0\n",
            "                   penis = True             spam : ham    =     42.6 : 1.0\n",
            "                   adobe = True             spam : ham    =     41.9 : 1.0\n",
            "                   risks = True             spam : ham    =     41.0 : 1.0\n",
            "                    lisa = True              ham : spam   =     39.7 : 1.0\n",
            "              affordable = True             spam : ham    =     39.4 : 1.0\n",
            "                 explode = True             spam : ham    =     39.4 : 1.0\n",
            "            solicitation = True             spam : ham    =     38.1 : 1.0\n",
            "                 massive = True             spam : ham    =     37.7 : 1.0\n",
            "             shareholder = True             spam : ham    =     37.7 : 1.0\n",
            "                   susan = True              ham : spam   =     37.2 : 1.0\n",
            "                   steve = True              ham : spam   =     36.4 : 1.0\n",
            "                     fat = True             spam : ham    =     36.1 : 1.0\n",
            "                  norton = True             spam : ham    =     36.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.text import Text\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def concordance(data_list, search_word):\n",
        "    \"\"\"\n",
        "    Displays the concordance of the specified search word in each email from the data list.\n",
        "\n",
        "    This function tokenizes each email's content to lowercase and creates an NLTK Text object.\n",
        "    If the search word is found in the token list, it displays its context using NLTK's concordance method.\n",
        "\n",
        "    Args:\n",
        "        data_list (list): List of email contents as strings.\n",
        "        search_word (str): The word to search for in the emails.\n",
        "    \"\"\"\n",
        "    for email in data_list:\n",
        "        # Tokenize email text and convert to lowercase\n",
        "        word_list = [word for word in word_tokenize(email.lower())]\n",
        "        text_list = Text(word_list)\n",
        "        if search_word in word_list:\n",
        "            text_list.concordance(search_word)\n",
        "\n",
        "print(\"STOCKS in HAM:\")\n",
        "concordance(ham_list, \"stocks\")\n",
        "\n",
        "print(\"\\n\\nSTOCKS in SPAM:\")\n",
        "concordance(spam_list, \"stocks\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYmLOAayMn6v",
        "outputId": "6f79079e-7354-4cab-a5f7-47de2b7acfbb"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "STOCKS in HAM:\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ur member directory . * follow your stocks and news headlines , exchange files\n",
            "Displaying 1 of 1 matches:\n",
            "ad my portfolio is diversified into stocks that have lost even more money than\n",
            "\n",
            "\n",
            "STOCKS in SPAM:\n",
            "Displaying 1 of 1 matches:\n",
            "ecializing in undervalued small cap stocks for immediate breakout erhc and exx\n",
            "Displaying 1 of 1 matches:\n",
            "in apple investments , inc profiled stocks . in order to be in full compliance\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "n how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 3 of 3 matches:\n",
            "5 how many times have you seen good stocks but you couldn ' t get your hands o\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "                    subject : penny stocks are about timing nomad internationa\n",
            " one trade friday ! go ndin . penny stocks are considered highiy speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "scovering value in natural resource stocks elgin resources ( elr - tsx ) extra\n",
            "Displaying 1 of 1 matches:\n",
            "fessionally not multi - level - not stocks - not real estate no cost tele - se\n",
            "Displaying 1 of 1 matches:\n",
            "ne trade thursday ! go fcdh . penny stocks are considered highiy specuiative a\n",
            "Displaying 4 of 4 matches:\n",
            "ck monday some of these little voip stocks have been rea | | y moving lately .\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy speculative a\n",
            "Displaying 5 of 5 matches:\n",
            "5 where were you when the following stocks exploded : scos : exploded from . 3\n",
            "d . 80 on friday . face it . little stocks can mean big gains for you . this r\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 2 of 2 matches:\n",
            "ims and do your own due diligence . stocks to play ( s 2 p ) profiles are not \n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been realiy moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
            "Displaying 4 of 4 matches:\n",
            "watch this one trade . these little stocks can surprise in a big way sometimes\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "nt opportunity drummond , small cap stocks alert newsletter must read - alert \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . - - - - - - - - - - - - - - - - -\n",
            "Displaying 3 of 3 matches:\n",
            " plays . widespread gains in energy stocks are inflating the portfolios of agg\n",
            "st levels of the year , with energy stocks outperforming all other market sect\n",
            "utions that sma | | and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 1 of 1 matches:\n",
            "dge - ksige are you tired of buying stocks and not having them perform ? our s\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 5 of 5 matches:\n",
            "ck monday some of these littie voip stocks have been really moving lately . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 2 of 2 matches:\n",
            "ck monday some of these little voip stocks have been really moving lately . an\n",
            " one trade monday ! go ypil . penny stocks are considered highiy specuiative a\n",
            "Displaying 2 of 2 matches:\n",
            "rt identifying defense and security stocks ready to explode look at the moves \n",
            " actual exchanges where small - cap stocks are traded . silica stopband doorkn\n",
            "Displaying 3 of 3 matches:\n",
            "report reveals this smallcap rocket stocks newsletter first we would like to s\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 6 of 6 matches:\n",
            " if you knew about these low priced stocks : otcbb : zapz : closed march 31 st\n",
            " following points : * many of these stocks are undiscovered and uncovered ! wh\n",
            " ! ! * * many of these undiscovered stocks are like coiled springs , wound tig\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 6 of 6 matches:\n",
            "hem : ( big money was made in these stocks by savvy investors who timed them r\n",
            "g filthy , stinking ri ' ch in tiny stocks no one has ever heard of until now \n",
            "ynamic things . some of these small stocks have absolutely exploded in price r\n",
            "`` occur . as with many micro - cap stocks , today ' s company has additional \n",
            " ema - il pertaining to investing , stocks or securities must be understood as\n",
            "ntative before deciding to trade in stocks featured within this ema - il . non\n",
            "Displaying 1 of 1 matches:\n",
            " receive first notice on run - away stocks traders ' monthly alert january pic\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "y agree , some , not all , of these stocks move in price because they are prom\n",
            "tands or that as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            " lose money from investing in penny stocks . if you wish to stop future mailin\n",
            "Displaying 4 of 4 matches:\n",
            "hree days . play of the week tracks stocks on downward trends , foresees botto\n",
            "mark is our uncanny ability to spot stocks that have bottomed - out and antici\n",
            "ound and upward trend . most of the stocks we track rebound and peak within ju\n",
            "om third party . investing in penny stocks is high risk and you should seek pr\n",
            "Displaying 4 of 4 matches:\n",
            "k tuesday some of these littie voip stocks have been reaily moving lateiy . an\n",
            " statements . as with many microcap stocks , today ' s company has additional \n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additional \n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 1 of 1 matches:\n",
            "or information puposes only . penny stocks are considered highly speculative a\n",
            "Displaying 1 of 1 matches:\n",
            "s obtained . investing in micro cap stocks is extremely risky and , investors \n",
            "Displaying 2 of 2 matches:\n",
            "ng their gains . select gold mining stocks are the hot flyers of the otc . his\n",
            "is letter cautions that micro - cap stocks are high - risk investments and tha\n",
            "Displaying 3 of 3 matches:\n",
            "might occur . as with many microcap stocks , today ' s company has additiona |\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "_ _ _ _ _ _ _ _ _ _ _ _ _ _ penny - stocks are considered highly speculative a\n",
            "Displaying 4 of 4 matches:\n",
            "n this stock . some of these smal | stocks are absoiuteiy fiying , as many of \n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "biication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 4 of 4 matches:\n",
            "tion is key to stock success rocket stocks newsletter u r g e n t i n v e s t \n",
            "ht occur . as with many micro - cap stocks , today ' s company has additional \n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            " the last 12 months , many of these stocks made triple and even quadruple retu\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 1 of 1 matches:\n",
            "the | ast 12 months , many of these stocks made triple and even quadruple retu\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 2 of 2 matches:\n",
            "is emai | pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 2 of 2 matches:\n",
            " % on regular price we have massive stocks of drugs for same day dispatch fast\n",
            "e do have the lowest price and huge stocks ready for same - day dispatch . two\n",
            "Displaying 1 of 1 matches:\n",
            "    subject : fwd : screw doctors . stocks available . vlagr @ . x _ a _ nax .\n",
            "Displaying 1 of 1 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go ndin . penny stocks are considered highly speculative a\n",
            "Displaying 2 of 2 matches:\n",
            " the last 12 months , many of these stocks made tripie and even quadruple retu\n",
            "one trade tuesday ! go mogi . penny stocks are considered highly speculative a\n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 3 of 3 matches:\n",
            "orage inc . play of the week tracks stocks on downward trends , foresees botto\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 1 of 1 matches:\n",
            "cautions that small and micro - cap stocks are high - risk investments and tha\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this emai | . none \n",
            "Displaying 5 of 5 matches:\n",
            "hursday ! some of these littie voip stocks have been realiy moving lateiy . an\n",
            "t can happen with these sma | | cap stocks when they take off . and it happens\n",
            " statements . as with many microcap stocks , today ' s company has additiona |\n",
            "is report pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this report . none \n",
            "Displaying 3 of 3 matches:\n",
            " statements . as with many microcap stocks , todays company has additional ris\n",
            "blication pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this publication . \n",
            "Displaying 3 of 3 matches:\n",
            "ancements but may be one of the few stocks left in this industry group that is\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 2 of 2 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "Displaying 4 of 4 matches:\n",
            "his email pertaining to investing , stocks , securities must be understood as \n",
            "ntative before deciding to trade in stocks featured within this email . none o\n",
            "eep in mind that when trading small stocks like the company above there is a c\n",
            "t professional before investing any stocks or mutual funds .\n",
            "Displaying 1 of 1 matches:\n",
            " one trade monday ! go wysk . penny stocks are considered highiy specuiative a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Contoh list email spam dan ham\n",
        "test_spam_list = [\"Participate in our new lottery!\", \"Try out this new medicine\"]\n",
        "test_ham_list = [\n",
        "    \"See the minutes from the last meeting attached\",\n",
        "    \"Investors are coming to our office on Monday\"\n",
        "]\n",
        "\n",
        "# Buat tuple (email_content, label) untuk masing-masing email\n",
        "test_emails = [(email_content, \"spam\") for email_content in test_spam_list]\n",
        "test_emails += [(email_content, \"ham\") for email_content in test_ham_list]\n",
        "\n",
        "# Ekstrak fitur untuk tiap email menggunakan fungsi get_features\n",
        "new_test_set = [(get_features(email), label) for (email, label) in test_emails]\n",
        "\n",
        "# Evaluasi classifier dengan training set yang sudah ada dan new test set\n",
        "evaluate(train_set, new_test_set, classifier)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EoJZ9v9BMvHn",
        "outputId": "8130834d-7c31-462f-c522-d3e8420320ba"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy on the training set = 0.957457094512932\n",
            "Accuracy on the test set = 1.0\n",
            "Most Informative Features\n",
            "               forwarded = True              ham : spam   =    197.1 : 1.0\n",
            "                     hou = True              ham : spam   =    186.4 : 1.0\n",
            "                    2004 = True             spam : ham    =    163.0 : 1.0\n",
            "            prescription = True             spam : ham    =    129.3 : 1.0\n",
            "                    pain = True             spam : ham    =     94.0 : 1.0\n",
            "                    2005 = True             spam : ham    =     90.8 : 1.0\n",
            "                    spam = True             spam : ham    =     89.1 : 1.0\n",
            "                     ect = True              ham : spam   =     82.9 : 1.0\n",
            "                  farmer = True              ham : spam   =     81.4 : 1.0\n",
            "                  differ = True             spam : ham    =     77.9 : 1.0\n",
            "                   super = True             spam : ham    =     77.9 : 1.0\n",
            "                featured = True             spam : ham    =     74.7 : 1.0\n",
            "              nomination = True              ham : spam   =     73.5 : 1.0\n",
            "                      ex = True             spam : ham    =     71.5 : 1.0\n",
            "                creative = True             spam : ham    =     68.3 : 1.0\n",
            "             medications = True             spam : ham    =     65.1 : 1.0\n",
            "                     713 = True              ham : spam   =     64.6 : 1.0\n",
            "                     ibm = True             spam : ham    =     61.8 : 1.0\n",
            "                   epson = True             spam : ham    =     57.0 : 1.0\n",
            "                    2001 = True              ham : spam   =     57.0 : 1.0\n",
            "                congress = True             spam : ham    =     55.4 : 1.0\n",
            "                  weight = True             spam : ham    =     55.4 : 1.0\n",
            "              complaints = True             spam : ham    =     53.8 : 1.0\n",
            "                    draw = True             spam : ham    =     53.8 : 1.0\n",
            "             legislation = True             spam : ham    =     53.8 : 1.0\n",
            "                     pro = True             spam : ham    =     53.8 : 1.0\n",
            "                 dealers = True             spam : ham    =     52.2 : 1.0\n",
            "                deciding = True             spam : ham    =     52.2 : 1.0\n",
            "              pertaining = True             spam : ham    =     52.2 : 1.0\n",
            "                   cheap = True             spam : ham    =     50.6 : 1.0\n",
            "                 doctors = True             spam : ham    =     49.0 : 1.0\n",
            "                  health = True             spam : ham    =     47.7 : 1.0\n",
            "                 foresee = True             spam : ham    =     45.8 : 1.0\n",
            "                      cc = True              ham : spam   =     44.4 : 1.0\n",
            "                 advises = True             spam : ham    =     44.2 : 1.0\n",
            "                 beliefs = True             spam : ham    =     44.2 : 1.0\n",
            "                     sum = True             spam : ham    =     44.2 : 1.0\n",
            "                   penis = True             spam : ham    =     42.6 : 1.0\n",
            "                   adobe = True             spam : ham    =     41.9 : 1.0\n",
            "                   risks = True             spam : ham    =     41.0 : 1.0\n",
            "                    lisa = True              ham : spam   =     39.7 : 1.0\n",
            "              affordable = True             spam : ham    =     39.4 : 1.0\n",
            "                 explode = True             spam : ham    =     39.4 : 1.0\n",
            "            solicitation = True             spam : ham    =     38.1 : 1.0\n",
            "                 massive = True             spam : ham    =     37.7 : 1.0\n",
            "             shareholder = True             spam : ham    =     37.7 : 1.0\n",
            "                   susan = True              ham : spam   =     37.2 : 1.0\n",
            "                   steve = True              ham : spam   =     36.4 : 1.0\n",
            "                     fat = True             spam : ham    =     36.1 : 1.0\n",
            "                  norton = True             spam : ham    =     36.1 : 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for email in test_spam_list:\n",
        "    print (email)\n",
        "    print (classifier.classify(get_features(email)))\n",
        "for email in test_ham_list:\n",
        "    print (email)\n",
        "    print (classifier.classify(get_features(email)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJ6peJDeM1T3",
        "outputId": "f5884bd6-3987-4eac-c99c-3100f860af8f"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Participate in our new lottery!\n",
            "spam\n",
            "Try out this new medicine\n",
            "spam\n",
            "See the minutes from the last meeting attached\n",
            "ham\n",
            "Investors are coming to our office on Monday\n",
            "ham\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    email = input(\"Type in your email here (or press 'Enter'): \")\n",
        "    if len(email)==0:\n",
        "        break\n",
        "    else:\n",
        "        prediction = classifier.classify(get_features(email))\n",
        "        print (f\"This email is likely {prediction}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HZ3G0pi7M4Gb",
        "outputId": "a92c8358-2c60-42f1-c05f-5897a04be1e1"
      },
      "execution_count": 62,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Type in your email here (or press 'Enter'): Buy new meds\n",
            "This email is likely spam\n",
            "\n",
            "Type in your email here (or press 'Enter'): Let's Schedule a meeting for tommorow\n",
            "This email is likely ham\n",
            "\n",
            "Type in your email here (or press 'Enter'): stock options fasts\n",
            "This email is likely spam\n",
            "\n",
            "Type in your email here (or press 'Enter'): investor are coming\n",
            "This email is likely ham\n",
            "\n",
            "Type in your email here (or press 'Enter'): 0\n",
            "This email is likely ham\n",
            "\n",
            "Type in your email here (or press 'Enter'): \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sAkD8fhMM6wI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}